{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: HuggingFace trainer.evaluate()\n",
    "* Fine-tuning dataset: stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/student/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q \"evaluate==0.4.3\"\n",
    "! pip install -q \"scikit-learn==1.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "BASE_MODEL = \"distilbert-base-uncased\"\n",
    "DATASET = \"stanfordnlp/imdb\"\n",
    "# PEFT_TECHNIQUE: \"LoRA\"\n",
    "# EVALUATION_APPROACH: HuggingFace trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058c9e424f86431484ede036bab3b1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 22.1MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 41.4MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 45.3MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7a871ad2b54717820fde3193cf6dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ecf44c51a04e8b99703f979606e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919876f0804448e19cc3a2a13c942962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split of dataset:\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 250\n",
      "})\n",
      "Test split of dataset:\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 250\n",
      "})\n",
      "Let's take a look at first data in the set:\n",
      "{'text': \"As soon as I heard about this film I knew I had to check it out. Well, I heard about it, then I found the trailer. After that, that's when I knew I had to see it. And I am so glad I did. You want to see classic television mixed with zombies? No? Then get lost.<br /><br />FIDO is a movie unlike anything I've ever seen. Well, actually, it kind of is. It's kind of like a Lassie episode and a Zombie film. Though when combined, it feels completely new and original. FIDO is about a little boy named Timmy and his new pet Fido. Well this new pet ain't no squawking parakeet or some potty-trained puppy. It's a re-animated dead guy...a zombie. A large radiation cloud engulfed Earth which led to all of the dead rising, which ensued the Zombie Wars. Though through the genius of Reinhold Giger, lead scientist of ZomCon, he discovered that if you destroy the brain, the zombie will perish, thus giving us the edge and the win in the Zombie War. Though due to lingering radiation, whoever dies becomes a zombie. Which can be a problem especially with the elderly. Though Zomcom steps up again with more breakthroughs, especially with the Domestication Collar. The collar stops the zombie's need for human flesh and thus making it harmless as a household pet. But not all is perfect in this Zombie Utopia, collars break, old people die and....well I'll just let you watch this incredibly unique flick.<br /><br />FIDO is a fantastic idea brought to fruition. With an all-star cast, and great writing FIDO rises above most in the comedy/horror genre. There are plenty of funny and original situations that really had me entertained. Though after seeing the film, I personally think the movie would have been better in black and white. At less than 90 minutes, the movie doesn't go on for too long and moves from scene to scene at a good rate. It'll probably end up being a cult-classic of sorts, since it's not really a laugh out loud comedy or even a horror movie. It's a comedy/family/zombie film immersed in the 1950 vibe. If you thought anything I said here was interesting by all means check this film out. But if you're still on the fence, swing your leg back over and stay there. 8.5 outta 10\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "loaded_dataset = load_dataset(DATASET, split=dataset_splits)\n",
    "datasets = {split: ds for split, ds in zip(dataset_splits, loaded_dataset)}\n",
    "\n",
    "# Thin out the dataset to make it run faster for this example\n",
    "for split in dataset_splits:\n",
    "    datasets[split] = datasets[split].shuffle(seed=23).select(range(250))\n",
    "\n",
    "print(\"Train split of dataset:\")\n",
    "print(datasets[\"train\"])\n",
    "\n",
    "print(\"Test split of dataset:\")\n",
    "print(datasets[\"test\"])\n",
    "\n",
    "print(\"Let's take a look at first data in the set:\")\n",
    "print(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f53512239c54eb094d1d812a1eeafbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ea5698b044424890a5db81cc6f8670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2ae2384c1d4a0aaefb1dbf8f1b56bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a11fb73f6d4730bcb35a4dfa37843f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57a0392d5c40c5b627cfe3cf983b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a192f28cb769446590fc9769a402f25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the first example of tokenized training set:\n",
      "[101, 2004, 2574, 2004, 1045, 2657, 2055, 2023, 2143, 1045, 2354, 1045, 2018, 2000, 4638, 2009, 2041, 1012, 2092, 1010, 1045, 2657, 2055, 2009, 1010, 2059, 1045, 2179, 1996, 9117, 1012, 2044, 2008, 1010, 2008, 1005, 1055, 2043, 1045, 2354, 1045, 2018, 2000, 2156, 2009, 1012, 1998, 1045, 2572, 2061, 5580, 1045, 2106, 1012, 2017, 2215, 2000, 2156, 4438, 2547, 3816, 2007, 14106, 1029, 2053, 1029, 2059, 2131, 2439, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 10882, 3527, 2003, 1037, 3185, 4406, 2505, 1045, 1005, 2310, 2412, 2464, 1012, 2092, 1010, 2941, 1010, 2009, 2785, 1997, 2003, 1012, 2009, 1005, 1055, 2785, 1997, 2066, 1037, 27333, 2666, 2792, 1998, 1037, 11798, 2143, 1012, 2295, 2043, 4117, 1010, 2009, 5683, 3294, 2047, 1998, 2434, 1012, 10882, 3527, 2003, 2055, 1037, 2210, 2879, 2315, 27217, 1998, 2010, 2047, 9004, 10882, 3527, 1012, 2092, 2023, 2047, 9004, 7110, 1005, 1056, 2053, 5490, 6692, 26291, 2075, 11498, 20553, 2102, 2030, 2070, 8962, 3723, 1011, 4738, 17022, 1012, 2009, 1005, 1055, 1037, 2128, 1011, 6579, 2757, 3124, 1012, 1012, 1012, 1037, 11798, 1012, 1037, 2312, 8249, 6112, 24692, 3011, 2029, 2419, 2000, 2035, 1997, 1996, 2757, 4803, 1010, 2029, 18942, 1996, 11798, 5233, 1012, 2295, 2083, 1996, 11067, 1997, 27788, 12640, 15453, 2121, 1010, 2599, 7155, 1997, 1062, 5358, 8663, 1010, 2002, 3603, 2008, 2065, 2017, 6033, 1996, 4167, 1010, 1996, 11798, 2097, 2566, 4509, 1010, 2947, 3228, 2149, 1996, 3341, 1998, 1996, 2663, 1999, 1996, 11798, 2162, 1012, 2295, 2349, 2000, 15304, 8249, 1010, 9444, 8289, 4150, 1037, 11798, 1012, 2029, 2064, 2022, 1037, 3291, 2926, 2007, 1996, 9750, 1012, 2295, 1062, 5358, 9006, 4084, 2039, 2153, 2007, 2062, 12687, 2015, 1010, 2926, 2007, 1996, 4968, 3370, 9127, 1012, 1996, 9127, 6762, 1996, 11798, 1005, 1055, 2342, 2005, 2529, 5771, 1998, 2947, 2437, 2009, 19741, 2004, 1037, 4398, 9004, 1012, 2021, 2025, 2035, 2003, 3819, 1999, 2023, 11798, 26425, 1010, 9127, 2015, 3338, 1010, 2214, 2111, 3280, 1998, 1012, 1012, 1012, 1012, 2092, 1045, 1005, 2222, 2074, 2292, 2017, 3422, 2023, 11757, 4310, 17312, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 10882, 3527, 2003, 1037, 10392, 2801, 2716, 2000, 5909, 3258, 1012, 2007, 2019, 2035, 1011, 2732, 3459, 1010, 1998, 2307, 3015, 10882, 3527, 9466, 2682, 2087, 1999, 1996, 4038, 1013, 5469, 6907, 1012, 2045, 2024, 7564, 1997, 6057, 1998, 2434, 8146, 2008, 2428, 2018, 2033, 21474, 1012, 2295, 2044, 3773, 1996, 2143, 1010, 1045, 7714, 2228, 1996, 3185, 2052, 2031, 2042, 2488, 1999, 2304, 1998, 2317, 1012, 2012, 2625, 2084, 3938, 2781, 1010, 1996, 3185, 2987, 1005, 1056, 2175, 2006, 2005, 2205, 2146, 1998, 5829, 2013, 3496, 2000, 3496, 2012, 1037, 2204, 3446, 1012, 2009, 1005, 2222, 2763, 2203, 2039, 2108, 1037, 8754, 1011, 4438, 1997, 11901, 1010, 2144, 2009, 1005, 1055, 2025, 2428, 1037, 4756, 2041, 5189, 4038, 2030, 2130, 1037, 5469, 3185, 1012, 2009, 1005, 1055, 1037, 4038, 1013, 2155, 1013, 11798, 2143, 26275, 1999, 1996, 3925, 21209, 1012, 2065, 2017, 2245, 2505, 1045, 2056, 2182, 2001, 5875, 2011, 2035, 2965, 4638, 2023, 2143, 2041, 1012, 2021, 2065, 102]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples\"\"\"\n",
    "    return auto_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "for split in dataset_splits:\n",
    "    tokenized_ds[split] = datasets[split].map(preprocess_function, batched=True)\n",
    "\n",
    "# Check that we tokenized the examples properly\n",
    "assert tokenized_ds[\"train\"][0][\"input_ids\"][:5] == [101, 2004, 2574, 2004, 1045]\n",
    "\n",
    "print(\"Show the first example of tokenized training set:\")\n",
    "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4f9caf60cb44728431b782b82dab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4687d19c3e42e5ab9fba7f11919559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af50abd20d454bbb944875a3b78b18b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae239d05e2e4414da693ad9089e6a4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classifiers:\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Inspect Model:\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    # quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "foundation_model.to(DEVICE)\n",
    "\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\", num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in foundation_model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Model classifiers:\")\n",
    "print(foundation_model.classifier)\n",
    "\n",
    "print(\"Inspect Model:\")\n",
    "print(foundation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ccdc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823334</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8233343362808228,\n",
       " 'eval_accuracy': 0.528,\n",
       " 'eval_runtime': 3.8631,\n",
       " 'eval_samples_per_second': 64.715,\n",
       " 'eval_steps_per_second': 16.308,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    e_predictions, e_labels = eval_predictions\n",
    "    e_predictions = np.argmax(e_predictions, axis=1)\n",
    "    return {\"accuracy\": (e_predictions == e_labels).mean()}\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=auto_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=auto_tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cf2107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  predicted_label\n",
      "0  I've seen this amusing little 'brit flick'many...      1                1\n",
      "1  Yep, Edward G. gives us a retro view of the cr...      0                1\n",
      "2  Has there ever been an Angel of Death like MIM...      1                1\n",
      "3  This is one of the worst Sandra Bullock movie ...      0                1\n",
      "4  Dr Steven Segal saves the world from a deadly ...      0                1\n",
      "5  An interesting concept turned into carnage... ...      0                1\n",
      "6  Not good. Mostly because you don't give a damn...      0                1\n",
      "7  The opening scene makes you feel like you're w...      0                1\n",
      "8  I've watched a number of Wixel Pixel and Sub R...      0                1\n",
      "9  Continuing in the string of \"stalker/slasher\" ...      0                1\n",
      "Some of the incorrect predictions:\n",
      "                                                 text  label  predicted_label\n",
      "1   Yep, Edward G. gives us a retro view of the cr...      0                1\n",
      "3   This is one of the worst Sandra Bullock movie ...      0                1\n",
      "4   Dr Steven Segal saves the world from a deadly ...      0                1\n",
      "5   An interesting concept turned into carnage... ...      0                1\n",
      "6   Not good. Mostly because you don't give a damn...      0                1\n",
      "7   The opening scene makes you feel like you're w...      0                1\n",
      "8   I've watched a number of Wixel Pixel and Sub R...      0                1\n",
      "9   Continuing in the string of \"stalker/slasher\" ...      0                1\n",
      "10  I have been a huge fan of the original crew of...      0                1\n",
      "11  I love Das Boot. I hoped for something along s...      0                1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"Some of the incorrect predictions:\")\n",
    "print(df[df[\"label\"] != df[\"predicted_label\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "PEFT MODEL:\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Creating a PEFT model\n",
    "# Using the PEFT config and foundation model, create a PEFT model.\n",
    "\n",
    "# Creating a PEFT model\n",
    "peft_model = get_peft_model(foundation_model, peft_config)\n",
    "peft_model.to(DEVICE)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"PEFT MODEL:\")\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.740400</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726556</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32, training_loss=0.7428619265556335, metrics={'train_runtime': 25.6539, 'train_samples_per_second': 19.49, 'train_steps_per_second': 1.247, 'total_flos': 67369703424000.0, 'train_loss': 0.7428619265556335, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis_peft\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=auto_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=auto_tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the lightweight fine-tuned model\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7265563011169434,\n",
       " 'eval_accuracy': 0.528,\n",
       " 'eval_runtime': 4.115,\n",
       " 'eval_samples_per_second': 60.753,\n",
       " 'eval_steps_per_second': 3.888,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the lightweight fine-tuned model\n",
    "peft_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  predicted_label\n",
      "0  I've seen this amusing little 'brit flick'many...      1                1\n",
      "1  Yep, Edward G. gives us a retro view of the cr...      0                1\n",
      "2  Has there ever been an Angel of Death like MIM...      1                1\n",
      "3  This is one of the worst Sandra Bullock movie ...      0                1\n",
      "4  Dr Steven Segal saves the world from a deadly ...      0                1\n",
      "5  An interesting concept turned into carnage... ...      0                1\n",
      "6  Not good. Mostly because you don't give a damn...      0                1\n",
      "7  The opening scene makes you feel like you're w...      0                1\n",
      "8  I've watched a number of Wixel Pixel and Sub R...      0                1\n",
      "9  Continuing in the string of \"stalker/slasher\" ...      0                1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model\n",
    "peft_model.save_pretrained(\"./data/peft_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed model from saved PEFT:\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig, AutoPeftModelForSequenceClassification\n",
    "\n",
    "peft_model_path = \"./data/peft_model\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "saved_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(peft_model_path)\n",
    "# saved_peft_model.to(DEVICE)\n",
    "\n",
    "print(\"Reconstructed model from saved PEFT:\")\n",
    "print(saved_peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 21\n",
      "})\n",
      "[101, 1045, 1005, 2310, 2464, 2023, 19142, 2210, 1005, 28101, 17312, 1005, 2116, 2335, 1012, 1996, 2069, 3291, 2003, 2049, 2747, 20165, 2006, 2678, 2030, 4966, 1012, 1045, 1005, 24529, 5121, 1037, 20127, 2005, 1037, 4966, 2713, 1012, 1996, 2172, 4771, 2957, 5207, 3248, 1005, 5061, 2100, 1005, 2019, 4654, 1011, 6986, 2137, 1010, 3005, 2074, 2042, 2207, 2013, 3827, 1010, 2002, 4858, 2370, 1037, 3105, 2004, 2019, 3751, 2937, 1999, 1037, 2924, 1010, 2009, 2035, 3632, 2092, 2127, 2002, 4858, 2370, 7861, 12618, 18450, 1999, 1037, 2924, 2002, 2923, 2007, 2010, 4654, 13675, 10698, 2229, 1010, 2585, 9152, 8159, 3248, 1996, 3040, 23356, 7332, 1010, 2049, 2019, 22249, 2210, 17083, 2361, 1010, 11504, 2996, 5033, 2030, 8133, 3016, 1010, 2097, 2272, 2000, 1996, 5343, 1012, 2298, 2041, 2005, 2198, 13919, 9082, 2077, 2002, 4930, 2009, 2502, 2007, 1005, 26822, 12734, 1005, 10642, 1997, 1996, 2439, 15745, 1005, 2935, 1997, 1996, 7635, 1005, 1999, 1037, 2235, 2535, 2004, 1037, 19805, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'text': \"I've seen this amusing little 'brit flick'many times. The only problem is Its currently unavailable on video or DVD. I'ts certainly a contender for a DVD release. The much missed Richard Jordan plays 'pinky' an Ex-pat American, whose Just been released from prison,he finds himself A job as an Electrician in a bank, it all goes well until he finds Himself Embroiled in a bank heist with his ex cronies, David Niven Plays the mastermind Ivan, Its an enjoyable little romp, hopefully studio canal or anchor bay, will come to the Rescue. Look out for john Rhys Davies Before he struck it big with 'shogun' Raiders of the lost Ark 'Lord Of The Rings' In a small role as a barrister,\", 'label': 1, 'input_ids': [101, 1045, 1005, 2310, 2464, 2023, 19142, 2210, 1005, 28101, 17312, 1005, 2116, 2335, 1012, 1996, 2069, 3291, 2003, 2049, 2747, 20165, 2006, 2678, 2030, 4966, 1012, 1045, 1005, 24529, 5121, 1037, 20127, 2005, 1037, 4966, 2713, 1012, 1996, 2172, 4771, 2957, 5207, 3248, 1005, 5061, 2100, 1005, 2019, 4654, 1011, 6986, 2137, 1010, 3005, 2074, 2042, 2207, 2013, 3827, 1010, 2002, 4858, 2370, 1037, 3105, 2004, 2019, 3751, 2937, 1999, 1037, 2924, 1010, 2009, 2035, 3632, 2092, 2127, 2002, 4858, 2370, 7861, 12618, 18450, 1999, 1037, 2924, 2002, 2923, 2007, 2010, 4654, 13675, 10698, 2229, 1010, 2585, 9152, 8159, 3248, 1996, 3040, 23356, 7332, 1010, 2049, 2019, 22249, 2210, 17083, 2361, 1010, 11504, 2996, 5033, 2030, 8133, 3016, 1010, 2097, 2272, 2000, 1996, 5343, 1012, 2298, 2041, 2005, 2198, 13919, 9082, 2077, 2002, 4930, 2009, 2502, 2007, 1005, 26822, 12734, 1005, 10642, 1997, 1996, 2439, 15745, 1005, 2935, 1997, 1996, 7635, 1005, 1999, 1037, 2235, 2535, 2004, 1037, 19805, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "peft_tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "items_indexes_for_manual_review = \\\n",
    "    [0, 1, 13, 25, 46, 51, 64, 88, 90, 118, 125, 131, 148, 154, 166, 179, 183, 199, 222, 233, 244]\n",
    "\n",
    "## Evaluating the model\n",
    "test_items = tokenized_ds[\"test\"].select(\n",
    "    items_indexes_for_manual_review\n",
    ")\n",
    "\n",
    "# print test_items columns.\n",
    "print(test_items)\n",
    "\n",
    "# Inspect details.\n",
    "for test_item in test_items:\n",
    "    print(test_item['input_ids'])\n",
    "    print(test_item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "items_for_manual_review = tokenized_ds[\"test\"].select(\n",
    "    items_indexes_for_manual_review\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for i in items_for_manual_review:\n",
    "    input_tokens = peft_tokenizer(i['text'], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = saved_peft_model(**input_tokens).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        predictions.append(predicted_class_id)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  predictions  labels\n",
      "0   I've seen this amusing little 'brit flick'many...            1       1\n",
      "1   Yep, Edward G. gives us a retro view of the cr...            1       0\n",
      "2   When you watch low budget horror movies as muc...            1       0\n",
      "3   So funny is the perfect way to describe this 1...            1       1\n",
      "4   THE CELL fascinated me at first glance. I was ...            1       1\n",
      "5   I never comment on a film, but I have to say t...            1       0\n",
      "6   Wesley Snipes is perfectly cast as Blade, a ha...            1       1\n",
      "7   This is the best work i have ever seen on tele...            1       1\n",
      "8   Mean spirited, and down right degrading adapta...            1       0\n",
      "9   I can understand how Barney can be annoying to...            1       1\n",
      "10  During a lifetime of seeing and enjoying thous...            1       0\n",
      "11  When I was in 7th grade(back in 1977), I was a...            1       1\n",
      "12  It's painfully obvious that the people who mad...            1       0\n",
      "13  This show has to be my favorite out of all the...            1       1\n",
      "14  This is the one major problem with this film, ...            1       0\n",
      "15  I am very interested in animal children and I ...            1       0\n",
      "16  The \"good news\" is that the circus is in town....            1       1\n",
      "17  Boring, utterly predictable soap opera. Mary T...            1       0\n",
      "18  The plot of this film is not complicated. A ve...            1       1\n",
      "19  after my daughter was born in 1983, i needed t...            1       1\n",
      "20  FOLLOWING the business coup of the year of 194...            1       1\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [item[\"text\"] for item in items_for_manual_review],\n",
    "        \"predictions\": predictions,\n",
    "        \"labels\": [item[\"label\"] for item in items_for_manual_review],\n",
    "    }\n",
    ")\n",
    "# Show all the cells\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [item[\"label\"] for item in items_for_manual_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78917c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a90b73e5740a5a45a8bbb6bd2d79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5714285714285714}\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate accurancy.\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=labels, predictions=predictions)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

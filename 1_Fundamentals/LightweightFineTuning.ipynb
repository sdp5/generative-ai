{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: \"distilbert-base-uncased\"\n",
    "* Evaluation approach: HuggingFace trainer.evaluate()\n",
    "* Fine-tuning dataset: \"stanfordnlp/imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q \"evaluate==0.4.3\"\n",
    "! pip install -q \"scikit-learn==1.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "DATASET = \"stanfordnlp/imdb\"\n",
    "DATASET_SEED = 23\n",
    "SUBSET_SIZE = 100\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"\n",
    "FINE_TUNED_MODEL = \"tashrifmahmud/sentiment_analysis_model_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Subset of the dataset created successfully.\n",
      "Train split of dataset:\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Test split of dataset:\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Let's take a look at first data in the set:\n",
      "{'text': \"As soon as I heard about this film I knew I had to check it out. Well, I heard about it, then I found the trailer. After that, that's when I knew I had to see it. And I am so glad I did. You want to see classic television mixed with zombies? No? Then get lost.<br /><br />FIDO is a movie unlike anything I've ever seen. Well, actually, it kind of is. It's kind of like a Lassie episode and a Zombie film. Though when combined, it feels completely new and original. FIDO is about a little boy named Timmy and his new pet Fido. Well this new pet ain't no squawking parakeet or some potty-trained puppy. It's a re-animated dead guy...a zombie. A large radiation cloud engulfed Earth which led to all of the dead rising, which ensued the Zombie Wars. Though through the genius of Reinhold Giger, lead scientist of ZomCon, he discovered that if you destroy the brain, the zombie will perish, thus giving us the edge and the win in the Zombie War. Though due to lingering radiation, whoever dies becomes a zombie. Which can be a problem especially with the elderly. Though Zomcom steps up again with more breakthroughs, especially with the Domestication Collar. The collar stops the zombie's need for human flesh and thus making it harmless as a household pet. But not all is perfect in this Zombie Utopia, collars break, old people die and....well I'll just let you watch this incredibly unique flick.<br /><br />FIDO is a fantastic idea brought to fruition. With an all-star cast, and great writing FIDO rises above most in the comedy/horror genre. There are plenty of funny and original situations that really had me entertained. Though after seeing the film, I personally think the movie would have been better in black and white. At less than 90 minutes, the movie doesn't go on for too long and moves from scene to scene at a good rate. It'll probably end up being a cult-classic of sorts, since it's not really a laugh out loud comedy or even a horror movie. It's a comedy/family/zombie film immersed in the 1950 vibe. If you thought anything I said here was interesting by all means check this film out. But if you're still on the fence, swing your leg back over and stay there. 8.5 outta 10\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset from the Hugging Face Hub.\n",
    "dataset_splits = [\"train\", \"test\"]\n",
    "try:\n",
    "    # Example: IMDB dataset for sequence classification\n",
    "    loaded_dataset = load_dataset(DATASET, split=dataset_splits)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "else:\n",
    "    datasets = {split: ds for split, ds in zip(dataset_splits, loaded_dataset)}\n",
    "\n",
    "# Take a subset of the dataset to reduce computational resources\n",
    "try:\n",
    "    # Thin out the dataset to make it run faster for this example.\n",
    "    for split in dataset_splits:\n",
    "        datasets[split] = datasets[split].shuffle(seed=DATASET_SEED).select(range(SUBSET_SIZE))\n",
    "    print(\"Subset of the dataset created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating a subset of the dataset: {e}\")\n",
    "else:\n",
    "    print(\"Train split of dataset:\")\n",
    "    print(datasets[\"train\"])\n",
    "\n",
    "    print(\"Test split of dataset:\")\n",
    "    print(datasets[\"test\"])\n",
    "\n",
    "    print(\"Let's take a look at first data in the set:\")\n",
    "    print(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d5539ca0794c9092ed8bff42f16c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized successfully.\n",
      "Show the first example of tokenized training set:\n",
      "[101, 2004, 2574, 2004, 1045, 2657, 2055, 2023, 2143, 1045, 2354, 1045, 2018, 2000, 4638, 2009, 2041, 1012, 2092, 1010, 1045, 2657, 2055, 2009, 1010, 2059, 1045, 2179, 1996, 9117, 1012, 2044, 2008, 1010, 2008, 1005, 1055, 2043, 1045, 2354, 1045, 2018, 2000, 2156, 2009, 1012, 1998, 1045, 2572, 2061, 5580, 1045, 2106, 1012, 2017, 2215, 2000, 2156, 4438, 2547, 3816, 2007, 14106, 1029, 2053, 1029, 2059, 2131, 2439, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 10882, 3527, 2003, 1037, 3185, 4406, 2505, 1045, 1005, 2310, 2412, 2464, 1012, 2092, 1010, 2941, 1010, 2009, 2785, 1997, 2003, 1012, 2009, 1005, 1055, 2785, 1997, 2066, 1037, 27333, 2666, 2792, 1998, 1037, 11798, 2143, 1012, 2295, 2043, 4117, 1010, 2009, 5683, 3294, 2047, 1998, 2434, 1012, 10882, 3527, 2003, 2055, 1037, 2210, 2879, 2315, 27217, 1998, 2010, 2047, 9004, 10882, 3527, 1012, 2092, 2023, 2047, 9004, 7110, 1005, 1056, 2053, 5490, 6692, 26291, 2075, 11498, 20553, 2102, 2030, 2070, 8962, 3723, 1011, 4738, 17022, 1012, 2009, 1005, 1055, 1037, 2128, 1011, 6579, 2757, 3124, 1012, 1012, 1012, 1037, 11798, 1012, 1037, 2312, 8249, 6112, 24692, 3011, 2029, 2419, 2000, 2035, 1997, 1996, 2757, 4803, 1010, 2029, 18942, 1996, 11798, 5233, 1012, 2295, 2083, 1996, 11067, 1997, 27788, 12640, 15453, 2121, 1010, 2599, 7155, 1997, 1062, 5358, 8663, 1010, 2002, 3603, 2008, 2065, 2017, 6033, 1996, 4167, 1010, 1996, 11798, 2097, 2566, 4509, 1010, 2947, 3228, 2149, 1996, 3341, 1998, 1996, 2663, 1999, 1996, 11798, 2162, 1012, 2295, 2349, 2000, 15304, 8249, 1010, 9444, 8289, 4150, 1037, 11798, 1012, 2029, 2064, 2022, 1037, 3291, 2926, 2007, 1996, 9750, 1012, 2295, 1062, 5358, 9006, 4084, 2039, 2153, 2007, 2062, 12687, 2015, 1010, 2926, 2007, 1996, 4968, 3370, 9127, 1012, 1996, 9127, 6762, 1996, 11798, 1005, 1055, 2342, 2005, 2529, 5771, 1998, 2947, 2437, 2009, 19741, 2004, 1037, 4398, 9004, 1012, 2021, 2025, 2035, 2003, 3819, 1999, 2023, 11798, 26425, 1010, 9127, 2015, 3338, 1010, 2214, 2111, 3280, 1998, 1012, 1012, 1012, 1012, 2092, 1045, 1005, 2222, 2074, 2292, 2017, 3422, 2023, 11757, 4310, 17312, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 10882, 3527, 2003, 1037, 10392, 2801, 2716, 2000, 5909, 3258, 1012, 2007, 2019, 2035, 1011, 2732, 3459, 1010, 1998, 2307, 3015, 10882, 3527, 9466, 2682, 2087, 1999, 1996, 4038, 1013, 5469, 6907, 1012, 2045, 2024, 7564, 1997, 6057, 1998, 2434, 8146, 2008, 2428, 2018, 2033, 21474, 1012, 2295, 2044, 3773, 1996, 2143, 1010, 1045, 7714, 2228, 1996, 3185, 2052, 2031, 2042, 2488, 1999, 2304, 1998, 2317, 1012, 2012, 2625, 2084, 3938, 2781, 1010, 1996, 3185, 2987, 1005, 1056, 2175, 2006, 2005, 2205, 2146, 1998, 5829, 2013, 3496, 2000, 3496, 2012, 1037, 2204, 3446, 1012, 2009, 1005, 2222, 2763, 2203, 2039, 2108, 1037, 8754, 1011, 4438, 1997, 11901, 1010, 2144, 2009, 1005, 1055, 2025, 2428, 1037, 4756, 2041, 5189, 4038, 2030, 2130, 1037, 5469, 3185, 1012, 2009, 1005, 1055, 1037, 4038, 1013, 2155, 1013, 11798, 2143, 26275, 1999, 1996, 3925, 21209, 1012, 2065, 2017, 2245, 2505, 1045, 2056, 2182, 2001, 5875, 2011, 2035, 2965, 4638, 2023, 2143, 2041, 1012, 2021, 2065, 102]\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for the pretrained model.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    pre_trained_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "    fine_tuned_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL)\n",
    "    print(\"Tokenizers loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the tokenizers: {e}\")\n",
    "\n",
    "# Tokenize both the dataset subsets: train and test.\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the imdb dataset by returning tokenized examples\"\"\"\n",
    "    return pre_trained_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_ds = {}\n",
    "try:\n",
    "    for split in dataset_splits:\n",
    "        tokenized_ds[split] = datasets[split].map(preprocess_function, batched=True)\n",
    "    print(\"Dataset tokenized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while tokenizing the dataset: {e}\")\n",
    "else:\n",
    "    # Check that we tokenized the examples properly\n",
    "    assert tokenized_ds[\"train\"][0][\"input_ids\"][:5] == [101, 2004, 2574, 2004, 1045]\n",
    "\n",
    "    print(\"Show the first example of tokenized training set:\")\n",
    "    print(tokenized_ds[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model loaded successfully.\n",
      "Pretrained Model classifiers:\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Inspect Pretrained Model:\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "try:\n",
    "    pre_trained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        PRETRAINED_MODEL,\n",
    "        # quantization_config=bnb_config,\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        num_labels=2,\n",
    "        id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # For converting predictions to strings\n",
    "        label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    )\n",
    "    print(\"Pretrained Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")\n",
    "else:\n",
    "    pre_trained_model.to(DEVICE)\n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in pre_trained_model.base_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Pretrained Model classifiers:\")\n",
    "print(pre_trained_model.classifier)\n",
    "\n",
    "print(\"Inspect Pretrained Model:\")\n",
    "print(pre_trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c3dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuned Model loaded successfully.\n",
      "Fine Tuned Model classifiers:\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "Inspect Fine Tuned Model:\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for sequence classification\n",
    "try:\n",
    "    fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        FINE_TUNED_MODEL, num_labels=2,\n",
    "        id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "        label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    )\n",
    "    print(\"Fine tuned Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")\n",
    "else:\n",
    "    fine_tuned_model.to(DEVICE)\n",
    "\n",
    "print(\"Fine Tuned Model classifiers:\")\n",
    "print(fine_tuned_model.classifier)\n",
    "\n",
    "print(\"Inspect Fine Tuned Model:\")\n",
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ab9ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate pre-train foundation_model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.701190173625946, 'eval_accuracy': 0.45, 'eval_runtime': 2.3774, 'eval_samples_per_second': 42.063, 'eval_steps_per_second': 10.516}\n",
      "Full train the foundation_model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692948</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/sentiment_analysis/pre_trained/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate post-train foundation_model.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6929481625556946, 'eval_accuracy': 0.53, 'eval_runtime': 1.6984, 'eval_samples_per_second': 58.88, 'eval_steps_per_second': 14.72, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "# Helper function to compute prediction accuracy\n",
    "def compute_metrics(eval_predictions):\n",
    "    e_predictions, e_labels = eval_predictions\n",
    "    e_predictions = np.argmax(e_predictions, axis=1)\n",
    "    return {\"accuracy\": (e_predictions == e_labels).mean()}\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=pre_trained_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis/pre_trained/\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=pre_trained_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=pre_trained_tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Evaluate pre-train foundation_model.\")\n",
    "pre_train_evaluate_results = trainer.evaluate()\n",
    "print(pre_train_evaluate_results)\n",
    "\n",
    "print(\"Full train the foundation_model.\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Evaluate post-train foundation_model.\")\n",
    "post_train_evaluate_results = trainer.evaluate()\n",
    "print(post_train_evaluate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e2eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate fine-tuned pre-trained model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18837320804595947, 'eval_accuracy': 0.94, 'eval_runtime': 1.712, 'eval_samples_per_second': 58.412, 'eval_steps_per_second': 29.206}\n"
     ]
    }
   ],
   "source": [
    "# The HuggingFace Trainer class for the fine_tuned_model.\n",
    "trainer_ft = Trainer(\n",
    "    model=fine_tuned_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis/fine_tuned/\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=fine_tuned_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=fine_tuned_tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Evaluate fine-tuned pre-trained model.\")\n",
    "fine_tuned_evaluate_results = trainer_ft.evaluate()\n",
    "print(fine_tuned_evaluate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b512f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  predicted_label\n",
      "0  I've seen this amusing little 'brit flick'many...      1                0\n",
      "1  Yep, Edward G. gives us a retro view of the cr...      0                0\n",
      "2  Has there ever been an Angel of Death like MIM...      1                0\n",
      "3  This is one of the worst Sandra Bullock movie ...      0                0\n",
      "4  Dr Steven Segal saves the world from a deadly ...      0                0\n",
      "5  An interesting concept turned into carnage... ...      0                0\n",
      "6  Not good. Mostly because you don't give a damn...      0                0\n",
      "7  The opening scene makes you feel like you're w...      0                0\n",
      "8  I've watched a number of Wixel Pixel and Sub R...      0                0\n",
      "9  Continuing in the string of \"stalker/slasher\" ...      0                0\n",
      "Look at some of the incorrect predictions.\n",
      "                                                 text  label  predicted_label\n",
      "0   I've seen this amusing little 'brit flick'many...      1                0\n",
      "2   Has there ever been an Angel of Death like MIM...      1                0\n",
      "12  I gave this a 10 because it's the best film of...      1                0\n",
      "15  Korean \"romance\" about the owner of a camera s...      1                0\n",
      "17  Ostensibly a story about the young child of Ji...      1                0\n",
      "20  The rise of punk music was scarcely documented...      1                0\n",
      "21  I love this film (dont know why it is called P...      1                0\n",
      "23  I just saw this movie tonight(5th Nov. 2005)fo...      1                0\n",
      "24  As a longtime admirer of the 2001 film \"Moulin...      1                0\n",
      "25  So funny is the perfect way to describe this 1...      1                0\n"
     ]
    }
   ],
   "source": [
    "# Show some of the predictions.\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces.\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe.\n",
    "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"Look at some of the incorrect predictions.\")\n",
    "print(df[df[\"label\"] != df[\"predicted_label\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n",
      "PEFT SEQ_CLS MODEL:\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a PEFT config with appropriate hyperparameters for the chosen model.\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Set up a LoRA config\n",
    "peft_config_seq_cls = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# CREATING A PEFT MODEL\n",
    "\n",
    "# Using the PEFT config and foundation model, create a PEFT model.\n",
    "peft_model_seq_cls = get_peft_model(pre_trained_model, peft_config_seq_cls)\n",
    "peft_model_seq_cls.to(DEVICE)\n",
    "peft_model_seq_cls.print_trainable_parameters()\n",
    "\n",
    "print(\"PEFT SEQ_CLS MODEL:\")\n",
    "print(peft_model_seq_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING THE MODEL\n",
    "\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "peft_trainer_seq_cls = Trainer(\n",
    "    model=peft_model_seq_cls,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis/peft_seq_cls/\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=pre_trained_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=pre_trained_tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_seq_cls_pre_train_results: {'eval_loss': 0.6929482221603394, 'eval_accuracy': 0.53, 'eval_runtime': 1.6689, 'eval_samples_per_second': 59.918, 'eval_steps_per_second': 4.194}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692711</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692493</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/sentiment_analysis/peft_seq_cls/checkpoint-7 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/sentiment_analysis/peft_seq_cls/checkpoint-14 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_seq_cls_post_train_results: {'eval_loss': 0.6924930810928345, 'eval_accuracy': 0.53, 'eval_runtime': 1.7085, 'eval_samples_per_second': 58.531, 'eval_steps_per_second': 4.097, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate pre-trained lightweight fine-tuned model.\n",
    "peft_seq_cls_pre_train_evaluate_results = peft_trainer_seq_cls.evaluate()\n",
    "print(f\"peft_seq_cls_pre_train_results: {peft_seq_cls_pre_train_evaluate_results}\")\n",
    "\n",
    "# Train the lightweight fine-tuned model.\n",
    "peft_trainer_seq_cls.train()\n",
    "\n",
    "# Evaluate the lightweight fine-tuned model post training.\n",
    "peft_seq_cls_post_train_evaluate_results = peft_trainer_seq_cls.evaluate()\n",
    "print(f\"peft_seq_cls_post_train_results: {peft_seq_cls_post_train_evaluate_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  predicted_label\n",
      "0  I've seen this amusing little 'brit flick'many...      1                0\n",
      "1  Yep, Edward G. gives us a retro view of the cr...      0                0\n",
      "2  Has there ever been an Angel of Death like MIM...      1                0\n",
      "3  This is one of the worst Sandra Bullock movie ...      0                0\n",
      "4  Dr Steven Segal saves the world from a deadly ...      0                0\n",
      "5  An interesting concept turned into carnage... ...      0                0\n",
      "6  Not good. Mostly because you don't give a damn...      0                0\n",
      "7  The opening scene makes you feel like you're w...      0                0\n",
      "8  I've watched a number of Wixel Pixel and Sub R...      0                0\n",
      "9  Continuing in the string of \"stalker/slasher\" ...      0                0\n"
     ]
    }
   ],
   "source": [
    "# Show some of the predictions.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "\n",
    "# Replace <br /> tags in the text with spaces\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
    "\n",
    "# Add the model predictions to the dataframe\n",
    "predictions = peft_trainer_seq_cls.predict(tokenized_ds[\"test\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE TRAINED MODEL\n",
    "\n",
    "peft_model_seq_cls.save_pretrained(\"./data/peft_model/seq_cls/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed model from saved PEFT:\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): MultiHeadSelfAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL\n",
    "\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load model and config from the persist storage.\n",
    "peft_seq_cls_model_path = \"./data/peft_model/seq_cls/\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_seq_cls_model_path)\n",
    "\n",
    "saved_peft_seq_cls_model = AutoPeftModelForSequenceClassification.from_pretrained(peft_seq_cls_model_path)\n",
    "# saved_peft_seq_cls_model.to(DEVICE)\n",
    "\n",
    "print(\"Reconstructed model from saved PEFT:\")\n",
    "print(saved_peft_seq_cls_model)\n",
    "\n",
    "# Load AutoTokenizer\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "items_indexes_for_manual_review = [0, 1, 13, 25, 46, 51, 64, 88, 90, 94, 95, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 12\n",
      "})\n",
      "[101, 1045, 1005, 2310, 2464, 2023, 19142, 2210, 1005, 28101, 17312, 1005, 2116, 2335, 1012, 1996, 2069, 3291, 2003, 2049, 2747, 20165, 2006, 2678, 2030, 4966, 1012, 1045, 1005, 24529, 5121, 1037, 20127, 2005, 1037, 4966, 2713, 1012, 1996, 2172, 4771, 2957, 5207, 3248, 1005, 5061, 2100, 1005, 2019, 4654, 1011, 6986, 2137, 1010, 3005, 2074, 2042, 2207, 2013, 3827, 1010, 2002, 4858, 2370, 1037, 3105, 2004, 2019, 3751, 2937, 1999, 1037, 2924, 1010, 2009, 2035, 3632, 2092, 2127, 2002, 4858, 2370, 7861, 12618, 18450, 1999, 1037, 2924, 2002, 2923, 2007, 2010, 4654, 13675, 10698, 2229, 1010, 2585, 9152, 8159, 3248, 1996, 3040, 23356, 7332, 1010, 2049, 2019, 22249, 2210, 17083, 2361, 1010, 11504, 2996, 5033, 2030, 8133, 3016, 1010, 2097, 2272, 2000, 1996, 5343, 1012, 2298, 2041, 2005, 2198, 13919, 9082, 2077, 2002, 4930, 2009, 2502, 2007, 1005, 26822, 12734, 1005, 10642, 1997, 1996, 2439, 15745, 1005, 2935, 1997, 1996, 7635, 1005, 1999, 1037, 2235, 2535, 2004, 1037, 19805, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'text': \"I've seen this amusing little 'brit flick'many times. The only problem is Its currently unavailable on video or DVD. I'ts certainly a contender for a DVD release. The much missed Richard Jordan plays 'pinky' an Ex-pat American, whose Just been released from prison,he finds himself A job as an Electrician in a bank, it all goes well until he finds Himself Embroiled in a bank heist with his ex cronies, David Niven Plays the mastermind Ivan, Its an enjoyable little romp, hopefully studio canal or anchor bay, will come to the Rescue. Look out for john Rhys Davies Before he struck it big with 'shogun' Raiders of the lost Ark 'Lord Of The Rings' In a small role as a barrister,\", 'label': 1, 'input_ids': [101, 1045, 1005, 2310, 2464, 2023, 19142, 2210, 1005, 28101, 17312, 1005, 2116, 2335, 1012, 1996, 2069, 3291, 2003, 2049, 2747, 20165, 2006, 2678, 2030, 4966, 1012, 1045, 1005, 24529, 5121, 1037, 20127, 2005, 1037, 4966, 2713, 1012, 1996, 2172, 4771, 2957, 5207, 3248, 1005, 5061, 2100, 1005, 2019, 4654, 1011, 6986, 2137, 1010, 3005, 2074, 2042, 2207, 2013, 3827, 1010, 2002, 4858, 2370, 1037, 3105, 2004, 2019, 3751, 2937, 1999, 1037, 2924, 1010, 2009, 2035, 3632, 2092, 2127, 2002, 4858, 2370, 7861, 12618, 18450, 1999, 1037, 2924, 2002, 2923, 2007, 2010, 4654, 13675, 10698, 2229, 1010, 2585, 9152, 8159, 3248, 1996, 3040, 23356, 7332, 1010, 2049, 2019, 22249, 2210, 17083, 2361, 1010, 11504, 2996, 5033, 2030, 8133, 3016, 1010, 2097, 2272, 2000, 1996, 5343, 1012, 2298, 2041, 2005, 2198, 13919, 9082, 2077, 2002, 4930, 2009, 2502, 2007, 1005, 26822, 12734, 1005, 10642, 1997, 1996, 2439, 15745, 1005, 2935, 1997, 1996, 7635, 1005, 1999, 1037, 2235, 2535, 2004, 1037, 19805, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# EVALUATING THE MODEL\n",
    "\n",
    "test_items = tokenized_ds[\"test\"].select(\n",
    "    items_indexes_for_manual_review\n",
    ")\n",
    "\n",
    "# Print test_items columns.\n",
    "print(test_items)\n",
    "\n",
    "# Inspect test items and input_ids.\n",
    "for test_item in test_items:\n",
    "    print(test_item['input_ids'])\n",
    "    print(test_item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels.\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "items_for_manual_review = tokenized_ds[\"test\"].select(\n",
    "    items_indexes_for_manual_review\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for i in items_for_manual_review:\n",
    "    input_tokens = peft_tokenizer(i['text'], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = saved_peft_seq_cls_model(**input_tokens).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        predictions.append(predicted_class_id)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  predictions  labels\n",
      "0   I've seen this amusing little 'brit flick'many...            1       1\n",
      "1   Yep, Edward G. gives us a retro view of the cr...            1       0\n",
      "2   When you watch low budget horror movies as muc...            1       0\n",
      "3   So funny is the perfect way to describe this 1...            1       1\n",
      "4   THE CELL fascinated me at first glance. I was ...            1       1\n",
      "5   I never comment on a film, but I have to say t...            1       0\n",
      "6   Wesley Snipes is perfectly cast as Blade, a ha...            1       1\n",
      "7   This is the best work i have ever seen on tele...            1       1\n",
      "8   Mean spirited, and down right degrading adapta...            1       0\n",
      "9   Possibly the worst movie I ever saw. The perso...            1       0\n",
      "10  There won't be one moment in this film where y...            1       1\n",
      "11  Persuaded by the 7.0 points in IMDb, which is ...            1       0\n"
     ]
    }
   ],
   "source": [
    "# Table text, prediction and labels for the selected items.\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [item[\"text\"] for item in items_for_manual_review],\n",
    "        \"predictions\": predictions,\n",
    "        \"labels\": [item[\"label\"] for item in items_for_manual_review],\n",
    "    }\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Deduce labels of the selected items.\n",
    "labels = [item[\"label\"] for item in items_for_manual_review]\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=labels, predictions=predictions)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c688f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              stage  \\\n",
      "0  Foundation model before training   \n",
      "1   Foundation model after training   \n",
      "2        PEFT model before training   \n",
      "3         PEFT model after training   \n",
      "4          Fine tuned trained model   \n",
      "\n",
      "                                     details eval_loss  \n",
      "0                    distilbert-base-uncased  0.701190  \n",
      "1                    distilbert-base-uncased  0.692948  \n",
      "2                    PEFT LoRA SEQ_CLS Model  0.692948  \n",
      "3                    PEFT LoRA SEQ_CLS Model  0.692493  \n",
      "4  tashrifmahmud/sentiment_analysis_model_v2  0.188373  \n"
     ]
    }
   ],
   "source": [
    "# SHOW PERFORMANCE IMPROVEMENTS\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"stage\": [\n",
    "            \"Foundation model before training\",\n",
    "            \"Foundation model after training\",\n",
    "            \"PEFT model before training\",\n",
    "            \"PEFT model after training\",\n",
    "            \"Fine tuned trained model\",\n",
    "        ],\n",
    "        \"details\": [\n",
    "            PRETRAINED_MODEL,\n",
    "            PRETRAINED_MODEL,\n",
    "            \"PEFT LoRA SEQ_CLS Model\",\n",
    "            \"PEFT LoRA SEQ_CLS Model\",\n",
    "            FINE_TUNED_MODEL,\n",
    "        ],\n",
    "        \"eval_loss\": map(lambda x: \"{:.6f}\".format(x), [\n",
    "            pre_train_evaluate_results[\"eval_loss\"],\n",
    "            post_train_evaluate_results[\"eval_loss\"],\n",
    "            peft_seq_cls_pre_train_evaluate_results[\"eval_loss\"],\n",
    "            peft_seq_cls_post_train_evaluate_results[\"eval_loss\"],\n",
    "            fine_tuned_evaluate_results[\"eval_loss\"],\n",
    "        ]),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print eval_loss results\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e73fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
